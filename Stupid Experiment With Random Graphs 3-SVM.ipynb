{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random_adjacency_matrix import gen_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FORCE_REGEN_SAMPLE = False\n",
    "SAMPLE_GRAPH_NODES, SAMPLE_GRAPH_EDGES = (10, 10), (10, 30)\n",
    "SAMPLES_TRAIN, SAMPLES_VALIDATE, SAMPLES_TEST = 100000, 200, 500\n",
    "BATCH = 200\n",
    "\n",
    "def wrap_dir(fp):\n",
    "    return fp\n",
    "\n",
    "_SAMPLE_TRAIN_FILE = wrap_dir('stupid_experiment_with_random_graphs_train.csv')\n",
    "_SAMPLE_VALIDATE_FILE = wrap_dir('stupid_experiment_with_random_graphs_validate.csv')\n",
    "_SAMPLE_TEST_FILE = wrap_dir('stupid_experiment_with_random_graphs_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not isinstance(SAMPLE_GRAPH_NODES, (tuple, list)):\n",
    "    SAMPLE_GRAPH_NODES = (SAMPLE_GRAPH_NODES, SAMPLE_GRAPH_NODES)\n",
    "    \n",
    "if not isinstance(SAMPLE_GRAPH_EDGES, (tuple, list)):\n",
    "    SAMPLE_GRAPH_NODES = (SAMPLE_GRAPH_EDGES, SAMPLE_GRAPH_EDGES)\n",
    "\n",
    "def parse_sample(sample):\n",
    "    nodes, matdef = sample[0], sample[1]\n",
    "    nodes = int(nodes)\n",
    "    adj_mat = np.reshape(np.asarray([int(v.strip()) for v in matdef.split(',')], dtype=np.int8), newshape=(nodes, nodes))\n",
    "    \n",
    "    # feature 1: total nodes\n",
    "    f1 = nodes\n",
    "    \n",
    "    # feature 2: the number of edges\n",
    "    f2 = sum(np.matrix.flatten(adj_mat))\n",
    "    \n",
    "    # feature 3: the number of 1-out-degree nodes\n",
    "    f3 = len([r for r in adj_mat if sum(r) == 1])\n",
    "    \n",
    "    return (adj_mat, f1, f2, f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _write_sample_file(fp, data):\n",
    "    with open(fp, 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='|')\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "def _read_sample_file(fp):\n",
    "    with open(fp, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter='|')\n",
    "        for row in reader:\n",
    "            yield row\n",
    "            \n",
    "def _regen_samples():\n",
    "    def _gen(num):\n",
    "        for _ in range(num):\n",
    "            nodes, adj_mat = gen_graph(SAMPLE_GRAPH_NODES, SAMPLE_GRAPH_EDGES, directed=True)\n",
    "            yield str(nodes), ','.join(str(v) for v in np.matrix.flatten(adj_mat))\n",
    "\n",
    "    train_data = _gen(SAMPLES_TRAIN)\n",
    "    _write_sample_file(_SAMPLE_TRAIN_FILE, train_data)\n",
    "    \n",
    "    validate_data = _gen(SAMPLES_VALIDATE)\n",
    "    _write_sample_file(_SAMPLE_VALIDATE_FILE, validate_data)\n",
    "    \n",
    "    test_data = _gen(SAMPLES_TEST)\n",
    "    _write_sample_file(_SAMPLE_TEST_FILE, test_data)\n",
    "            \n",
    "\n",
    "if (\n",
    "    not os.path.exists(_SAMPLE_TRAIN_FILE) \n",
    "    or not os.path.exists(_SAMPLE_VALIDATE_FILE)\n",
    "    or not os.path.exists(_SAMPLE_TEST_FILE)\n",
    "    or FORCE_REGEN_SAMPLE\n",
    "):\n",
    "    #_regen_samples()\n",
    "    pass\n",
    "    \n",
    "\n",
    "def sample_batch_from_file(fp, batch_size=100):\n",
    "    done = False\n",
    "    reader = _read_sample_file(fp)\n",
    "    while not done:\n",
    "        batch_adj_mat_with_f, batch_y = [], []\n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                adj_mat, *f = parse_sample(next(reader))\n",
    "                bf = adj_mat\n",
    "                ident = np.identity(f[0])\n",
    "                bf = np.asarray([bf, ident, ident])\n",
    "                batch_adj_mat_with_f.append(bf)\n",
    "                batch_y.append(f)\n",
    "            except StopIteration:\n",
    "                done = True\n",
    "                del reader\n",
    "                break\n",
    "        yield batch_adj_mat_with_f, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(1.0, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    # tf.nn.conv2d对一个四维输入做二维卷积。\n",
    "    # 参数分别为输入，过滤器，步长，补0策略\n",
    "    # 其中：\n",
    "    # 输入的四个维度分别为：样本数，每样本高度，每样本宽度，通道数\n",
    "    # 过滤器的四个维度分别为：过滤器高度，过滤器宽度，输入通道数，输出通道数\n",
    "    # 步长为一个一维四元素张量，每元素分别表示在输入的对应维度上的步长\n",
    "    # 补0策略为'SAME'或'VALID'\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # tf.nn.max_pool对输入进行最大池化\n",
    "    # 参数分别为输入，窗口大小，步长，补0策略\n",
    "    # \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "max_nodes = SAMPLE_GRAPH_NODES[1]\n",
    "features = 3\n",
    "\n",
    "# Input adjacency matrices (samples, height, width)\n",
    "input_adj_mat_with_f = tf.placeholder(tf.float32, [None, 3, 10, 10])\n",
    "input_flat = tf.reshape(input_adj_mat_with_f, [-1, 300])\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "# First layer\n",
    "W_1 = weight_variable([300, 300])\n",
    "b_1 = bias_variable([300])\n",
    "h_1 = tf.nn.sigmoid(tf.matmul(input_flat, W_1) + b_1)\n",
    "\n",
    "# Second layer\n",
    "W_2 = weight_variable([300, 64])\n",
    "b_2 = bias_variable([64])\n",
    "h_2 = tf.nn.sigmoid(tf.matmul(h_1, W_2) + b_2)\n",
    "\n",
    "# Third layer\n",
    "W_3 = weight_variable([64, 64])\n",
    "b_3 = bias_variable([64])\n",
    "h_3 = tf.nn.sigmoid(tf.matmul(h_2, W_3) + b_3)\n",
    "\n",
    "# h_pool2 = max_pool_2x2(h_2)\n",
    "\n",
    "# Densely connected layer\n",
    "W_d = weight_variable([64, 1024])\n",
    "b_d = bias_variable([1024])\n",
    "h_d = tf.nn.sigmoid(tf.matmul(h_3, W_d) + b_d)\n",
    "\n",
    "# Readout layer\n",
    "W_r = weight_variable([1024, 3])\n",
    "b_r = bias_variable([3])\n",
    "\n",
    "y_conv = tf.matmul(h_d, W_r) + b_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "#cross_entropy = tf.reduce_mean(\n",
    "#    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=relu_3))\n",
    "c1 = y_ - y_conv\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(tf.reduce_sum(tf.multiply(c1, c1)))\n",
    "#train_step = tf.train.GradientDescentOptimizer(1e-5).minimize(tf.reduce_sum(tf.square(c1)))\n",
    "correct_prediction = tf.equal(tf.cast(tf.round(y_conv), tf.int32), tf.cast(y_, tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# launch the model\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: accuracy: 0.756667\n",
      "Step 10: accuracy: 0.78\n",
      "Step 20: accuracy: 0.8\n",
      "Step 30: accuracy: 0.786667\n",
      "Step 40: accuracy: 0.763333\n",
      "Step 50: accuracy: 0.773333\n",
      "Step 60: accuracy: 0.78\n",
      "Step 70: accuracy: 0.78\n",
      "Step 80: accuracy: 0.77\n",
      "Step 90: accuracy: 0.766667\n",
      "Step 100: accuracy: 0.776667\n",
      "Step 110: accuracy: 0.79\n",
      "Step 120: accuracy: 0.77\n",
      "Step 130: accuracy: 0.756667\n",
      "Step 140: accuracy: 0.793333\n",
      "Step 150: accuracy: 0.76\n",
      "Step 160: accuracy: 0.763333\n",
      "Step 170: accuracy: 0.76\n",
      "Step 180: accuracy: 0.77\n",
      "Step 190: accuracy: 0.79\n",
      "Step 200: accuracy: 0.773333\n",
      "Step 210: accuracy: 0.783333\n",
      "Step 220: accuracy: 0.773333\n",
      "Step 230: accuracy: 0.776667\n",
      "Step 240: accuracy: 0.783333\n",
      "Step 250: accuracy: 0.8\n",
      "Step 260: accuracy: 0.746667\n",
      "Step 270: accuracy: 0.763333\n",
      "Step 280: accuracy: 0.77\n",
      "Step 290: accuracy: 0.776667\n",
      "Step 300: accuracy: 0.78\n",
      "Step 310: accuracy: 0.773333\n",
      "Step 320: accuracy: 0.77\n",
      "Step 330: accuracy: 0.756667\n",
      "Step 340: accuracy: 0.77\n",
      "Step 350: accuracy: 0.76\n",
      "Step 360: accuracy: 0.783333\n",
      "Step 370: accuracy: 0.773333\n",
      "Step 380: accuracy: 0.796667\n",
      "Step 390: accuracy: 0.796667\n",
      "Step 400: accuracy: 0.773333\n",
      "Step 410: accuracy: 0.756667\n",
      "Step 420: accuracy: 0.77\n",
      "Step 430: accuracy: 0.786667\n",
      "Step 440: accuracy: 0.783333\n",
      "Step 450: accuracy: 0.786667\n",
      "Step 460: accuracy: 0.806667\n",
      "Step 470: accuracy: 0.763333\n",
      "Step 480: accuracy: 0.783333\n",
      "Step 490: accuracy: 0.776667\n",
      "Step 500: accuracy: 0.776667\n",
      "Step 510: accuracy: 0.78\n",
      "Step 520: accuracy: 0.76\n",
      "Step 530: accuracy: 0.786667\n",
      "Step 540: accuracy: 0.763333\n",
      "Step 550: accuracy: 0.77\n",
      "Step 560: accuracy: 0.766667\n",
      "Step 570: accuracy: 0.766667\n",
      "Step 580: accuracy: 0.81\n",
      "Step 590: accuracy: 0.79\n",
      "Step 600: accuracy: 0.773333\n",
      "Step 610: accuracy: 0.74\n",
      "Step 620: accuracy: 0.773333\n",
      "Step 630: accuracy: 0.773333\n",
      "Step 640: accuracy: 0.796667\n",
      "Step 650: accuracy: 0.793333\n",
      "Step 660: accuracy: 0.753333\n",
      "Step 670: accuracy: 0.763333\n",
      "Step 680: accuracy: 0.786667\n",
      "Step 690: accuracy: 0.793333\n",
      "Step 700: accuracy: 0.74\n",
      "Step 710: accuracy: 0.793333\n",
      "Step 720: accuracy: 0.77\n",
      "Step 730: accuracy: 0.77\n",
      "Step 740: accuracy: 0.763333\n",
      "Step 750: accuracy: 0.813333\n",
      "Step 760: accuracy: 0.756667\n",
      "Step 770: accuracy: 0.77\n",
      "Step 780: accuracy: 0.77\n",
      "Step 790: accuracy: 0.76\n",
      "Step 800: accuracy: 0.746667\n",
      "Step 810: accuracy: 0.79\n",
      "Step 820: accuracy: 0.756667\n",
      "Step 830: accuracy: 0.753333\n",
      "Step 840: accuracy: 0.766667\n",
      "Step 850: accuracy: 0.73\n",
      "Step 860: accuracy: 0.763333\n",
      "Step 870: accuracy: 0.79\n",
      "Step 880: accuracy: 0.746667\n",
      "Step 890: accuracy: 0.78\n",
      "Step 900: accuracy: 0.773333\n",
      "Step 910: accuracy: 0.813333\n",
      "Step 920: accuracy: 0.776667\n",
      "Step 930: accuracy: 0.76\n",
      "Step 940: accuracy: 0.77\n",
      "Step 950: accuracy: 0.76\n",
      "Step 960: accuracy: 0.746667\n",
      "Step 970: accuracy: 0.796667\n",
      "Step 980: accuracy: 0.773333\n",
      "Step 990: accuracy: 0.756667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch = sample_batch_from_file(_SAMPLE_TRAIN_FILE, batch_size=BATCH)\n",
    "\n",
    "for i in range(SAMPLES_TRAIN // BATCH):\n",
    "    try:\n",
    "        batch_adj_mat_with_f, batch_y = next(batch)\n",
    "    except StopIteration:\n",
    "        batch = sample_batch_from_file(_SAMPLE_TRAIN_FILE, batch_size=BATCH)\n",
    "        batch_adj_mat_with_f, batch_y = next(batch)\n",
    "    if len(batch_adj_mat_with_f) == 0:\n",
    "        continue\n",
    "    if i % 10 == 0:\n",
    "        acc = accuracy.eval(feed_dict={input_adj_mat_with_f: batch_adj_mat_with_f, \n",
    "                                       y_: batch_y})\n",
    "        print(\"Step %d: accuracy: %g\" % (i, acc))\n",
    "    sess.run(train_step, feed_dict={input_adj_mat_with_f: batch_adj_mat_with_f, \n",
    "                                    y_: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: [10, 19, 3]; Actual: [ 10.01219463  19.00857735   2.73004675]\n",
      "Expected: [10, 29, 1]; Actual: [  9.9787302   29.13644981   1.12909138]\n",
      "Expected: [10, 20, 2]; Actual: [ 10.00915909  20.03309441   2.61464405]\n",
      "Expected: [10, 20, 2]; Actual: [  9.94487762  20.13905525   2.47780275]\n",
      "Expected: [10, 12, 6]; Actual: [  9.99090195  12.03159523   3.87247562]\n",
      "Expected: [10, 27, 1]; Actual: [  9.95919895  27.08016205   1.37922049]\n",
      "Expected: [10, 19, 4]; Actual: [  9.96992016  19.00794601   2.8632412 ]\n",
      "Expected: [10, 12, 4]; Actual: [  9.96939182  11.9802084    3.79472828]\n",
      "Expected: [10, 22, 4]; Actual: [ 10.03061581  22.06005859   2.22525501]\n",
      "Expected: [10, 14, 4]; Actual: [  9.99098396  13.94580555   3.5934701 ]\n",
      "Expected: [10, 17, 1]; Actual: [  9.94874382  17.01911354   3.09948969]\n",
      "Expected: [10, 22, 3]; Actual: [ 10.00641441  22.14943504   2.21017385]\n",
      "Expected: [10, 20, 3]; Actual: [  9.95440388  20.064394     2.45864677]\n",
      "Expected: [10, 11, 3]; Actual: [  9.99105644  11.00770187   3.99887228]\n",
      "Expected: [10, 15, 2]; Actual: [  9.96566391  14.99894333   3.51182985]\n",
      "Expected: [10, 17, 3]; Actual: [  9.98970509  16.98953819   3.08890867]\n",
      "Expected: [10, 24, 2]; Actual: [  9.97662926  24.12320137   1.84557676]\n",
      "Expected: [10, 24, 2]; Actual: [  9.97317886  23.9527092    1.96238148]\n",
      "Expected: [10, 19, 3]; Actual: [  9.95119858  18.98633957   2.75934267]\n",
      "Expected: [10, 22, 5]; Actual: [  9.98890591  22.03274155   2.20396376]\n"
     ]
    }
   ],
   "source": [
    "batch_validate = sample_batch_from_file(_SAMPLE_TEST_FILE, batch_size=20)\n",
    "\n",
    "batch_adj_mat_val_with_f, batch_y_val = next(batch_validate)\n",
    "result = sess.run(y_conv, feed_dict={input_adj_mat_with_f: batch_adj_mat_val_with_f,\n",
    "                                       y_: batch_y_val\n",
    "                                       })\n",
    "\n",
    "for idx, x in enumerate(batch_y_val):\n",
    "    print(\"Expected: %s; Actual: %s\" % (x, result[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
