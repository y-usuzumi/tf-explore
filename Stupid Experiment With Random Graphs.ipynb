{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random_adjacency_matrix import gen_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_REGEN_SAMPLE = False\n",
    "SAMPLE_GRAPH_NODES, SAMPLE_GRAPH_EDGES = (10, 10), (10, 30)\n",
    "SAMPLES_TRAIN, SAMPLES_VALIDATE, SAMPLES_TEST = 100000, 200, 500\n",
    "BATCH = 2\n",
    "\n",
    "def wrap_dir(fp):\n",
    "    return fp\n",
    "\n",
    "_SAMPLE_TRAIN_FILE = wrap_dir('stupid_experiment_with_random_graphs_train.csv')\n",
    "_SAMPLE_VALIDATE_FILE = wrap_dir('stupid_experiment_with_random_graphs_validate.csv')\n",
    "_SAMPLE_TEST_FILE = wrap_dir('stupid_experiment_with_random_graphs_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(SAMPLE_GRAPH_NODES, (tuple, list)):\n",
    "    SAMPLE_GRAPH_NODES = (SAMPLE_GRAPH_NODES, SAMPLE_GRAPH_NODES)\n",
    "    \n",
    "if not isinstance(SAMPLE_GRAPH_EDGES, (tuple, list)):\n",
    "    SAMPLE_GRAPH_NODES = (SAMPLE_GRAPH_EDGES, SAMPLE_GRAPH_EDGES)\n",
    "\n",
    "def parse_sample(sample):\n",
    "    nodes, matdef = sample[0], sample[1]\n",
    "    nodes = int(nodes)\n",
    "    adj_mat = np.reshape(np.asarray([int(v.strip()) for v in matdef.split(',')], dtype=np.int8), newshape=(nodes, nodes))\n",
    "    \n",
    "    # feature 1: total nodes\n",
    "    f1 = nodes\n",
    "    \n",
    "    # feature 2: the number of edges\n",
    "    f2 = sum(np.matrix.flatten(adj_mat))\n",
    "    \n",
    "    # feature 3: the number of 1-out-degree nodes\n",
    "    f3 = len([r for r in adj_mat if sum(r) == 1])\n",
    "    \n",
    "    return (adj_mat, f1, f2, f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_sample_file(fp, data):\n",
    "    with open(fp, 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='|')\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "def _read_sample_file(fp):\n",
    "    with open(fp, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter='|')\n",
    "        for row in reader:\n",
    "            yield row\n",
    "            \n",
    "def _regen_samples():\n",
    "    def _gen(num):\n",
    "        for _ in range(num):\n",
    "            nodes, adj_mat = gen_graph(SAMPLE_GRAPH_NODES, SAMPLE_GRAPH_EDGES, directed=True)\n",
    "            yield str(nodes), ','.join(str(v) for v in np.matrix.flatten(adj_mat))\n",
    "\n",
    "    train_data = _gen(SAMPLES_TRAIN)\n",
    "    _write_sample_file(_SAMPLE_TRAIN_FILE, train_data)\n",
    "    \n",
    "    validate_data = _gen(SAMPLES_VALIDATE)\n",
    "    _write_sample_file(_SAMPLE_VALIDATE_FILE, validate_data)\n",
    "    \n",
    "    test_data = _gen(SAMPLES_TEST)\n",
    "    _write_sample_file(_SAMPLE_TEST_FILE, test_data)\n",
    "            \n",
    "\n",
    "if (\n",
    "    not os.path.exists(_SAMPLE_TRAIN_FILE) \n",
    "    or not os.path.exists(_SAMPLE_VALIDATE_FILE)\n",
    "    or not os.path.exists(_SAMPLE_TEST_FILE)\n",
    "    or FORCE_REGEN_SAMPLE\n",
    "):\n",
    "    #_regen_samples()\n",
    "    pass\n",
    "    \n",
    "\n",
    "def sample_batch_from_file(fp, batch_size=100):\n",
    "    done = False\n",
    "    reader = _read_sample_file(fp)\n",
    "    while not done:\n",
    "        batch_adj_mat, batch_f, batch_y = [], [], []\n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                adj_mat, *f = parse_sample(next(reader))\n",
    "                for i in range(f[0]):\n",
    "                    adj_mat[i, i] = 1\n",
    "                batch_adj_mat.append(adj_mat)\n",
    "                batch_f.append(np.ones((f[0], features)))\n",
    "                batch_y.append([list(f)])\n",
    "            except StopIteration:\n",
    "                done = True\n",
    "                break\n",
    "        yield batch_adj_mat, batch_f, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "max_nodes = SAMPLE_GRAPH_NODES[1]\n",
    "features = 3\n",
    "\n",
    "# Input adjacency matrices (samples, height, width)\n",
    "input_adj_mat = tf.placeholder(tf.float32, [None, None, None])\n",
    "input_f = tf.placeholder(tf.float32, [None, None, 3])\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 1, 3])\n",
    "\n",
    "# First layer\n",
    "A_x_h_1 = tf.matmul(input_adj_mat, input_f)\n",
    "W_1 = weight_variable([3, 128])\n",
    "b_1 = bias_variable([128])\n",
    "h_1 = tf.map_fn(lambda s: tf.matmul(s, W_1), A_x_h_1) + b_1\n",
    "\n",
    "relu_1 = tf.nn.relu(h_1)\n",
    "\n",
    "# Second layer\n",
    "W_2_z = weight_variable([1, 10])\n",
    "W_2 = weight_variable([128, 128])\n",
    "b_2 = bias_variable([128])\n",
    "h_2 = tf.map_fn(lambda s: tf.matmul(W_2_z, tf.matmul(s, W_2)), relu_1) + b_2\n",
    "relu_2 = tf.nn.relu(h_2)\n",
    "\n",
    "# Readout layer\n",
    "W_3 = weight_variable([64, 3])\n",
    "b_3 = bias_variable([3])\n",
    "h_3 = tf.map_fn(lambda s: tf.matmul(s, W_3), relu_2) + b_3\n",
    "relu_3 = h_3\n",
    "\n",
    "\n",
    "# Train\n",
    "#cross_entropy = tf.reduce_mean(\n",
    "#    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=relu_3))\n",
    "c1 = y_ - relu_3\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(tf.reduce_sum(tf.multiply(c1, c1)))\n",
    "#train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.cast(relu_3, tf.int32), tf.cast(y_, tf.int32))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "accuracy = relu_3\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# launch the model\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "batch = sample_batch_from_file(_SAMPLE_TRAIN_FILE, batch_size=BATCH)\n",
    "\n",
    "for _ in range(SAMPLES_TRAIN // BATCH):\n",
    "    batch_adj_mat, batch_f, batch_y = next(batch)\n",
    "    sess.run(train_step, feed_dict={input_adj_mat: batch_adj_mat, \n",
    "                                    input_f: batch_f, \n",
    "                                    y_: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: [10, 19, 3]; Actual: [  9.99717999  18.99988747   2.59805298]\n",
      "Expected: [10, 29, 1]; Actual: [  9.96837902  28.99187279   0.73597771]\n",
      "Expected: [10, 20, 2]; Actual: [  9.98408985  19.99081039   2.40075922]\n",
      "Expected: [10, 20, 2]; Actual: [  9.98507977  20.02750969   2.39527082]\n",
      "Expected: [10, 12, 6]; Actual: [  9.98909569  11.96247768   3.87451768]\n",
      "Expected: [10, 27, 1]; Actual: [  9.99398422  26.9720726    1.13679206]\n",
      "Expected: [10, 19, 4]; Actual: [  9.99696636  18.97505188   2.6023283 ]\n",
      "Expected: [10, 12, 4]; Actual: [  9.88932323  12.07290173   3.73122549]\n",
      "Expected: [10, 22, 4]; Actual: [  9.95916748  21.99634171   2.0033958 ]\n",
      "Expected: [10, 14, 4]; Actual: [ 10.06159687  13.92053127   3.60603976]\n"
     ]
    }
   ],
   "source": [
    "batch_validate = sample_batch_from_file(_SAMPLE_TEST_FILE, batch_size=10)\n",
    "\n",
    "batch_adj_mat_val, batch_f_val, batch_y_val = next(batch_validate)\n",
    "result = sess.run(accuracy, feed_dict={input_adj_mat: batch_adj_mat_val,\n",
    "                              input_f: batch_f_val,\n",
    "                              y_: batch_y_val})\n",
    "\n",
    "for idx, x in enumerate(batch_y_val):\n",
    "    print(\"Expected: %s; Actual: %s\" % (x[0], result[idx][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
