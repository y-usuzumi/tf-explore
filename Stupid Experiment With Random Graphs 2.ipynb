{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random_adjacency_matrix import gen_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FORCE_REGEN_SAMPLE = False\n",
    "SAMPLE_GRAPH_NODES, SAMPLE_GRAPH_EDGES = (10, 10), (10, 30)\n",
    "SAMPLES_TRAIN, SAMPLES_VALIDATE, SAMPLES_TEST = 100000, 200, 500\n",
    "BATCH = 50\n",
    "\n",
    "def wrap_dir(fp):\n",
    "    return fp\n",
    "\n",
    "_SAMPLE_TRAIN_FILE = wrap_dir('stupid_experiment_with_random_graphs_train.csv')\n",
    "_SAMPLE_VALIDATE_FILE = wrap_dir('stupid_experiment_with_random_graphs_validate.csv')\n",
    "_SAMPLE_TEST_FILE = wrap_dir('stupid_experiment_with_random_graphs_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not isinstance(SAMPLE_GRAPH_NODES, (tuple, list)):\n",
    "    SAMPLE_GRAPH_NODES = (SAMPLE_GRAPH_NODES, SAMPLE_GRAPH_NODES)\n",
    "    \n",
    "if not isinstance(SAMPLE_GRAPH_EDGES, (tuple, list)):\n",
    "    SAMPLE_GRAPH_NODES = (SAMPLE_GRAPH_EDGES, SAMPLE_GRAPH_EDGES)\n",
    "\n",
    "def parse_sample(sample):\n",
    "    nodes, matdef = sample[0], sample[1]\n",
    "    nodes = int(nodes)\n",
    "    adj_mat = np.reshape(np.asarray([int(v.strip()) for v in matdef.split(',')], dtype=np.int8), newshape=(nodes, nodes))\n",
    "    \n",
    "    # feature 1: total nodes\n",
    "    f1 = nodes\n",
    "    \n",
    "    # feature 2: the number of edges\n",
    "    f2 = sum(np.matrix.flatten(adj_mat))\n",
    "    \n",
    "    # feature 3: the number of 1-out-degree nodes\n",
    "    f3 = len([r for r in adj_mat if sum(r) == 1])\n",
    "    \n",
    "    return (adj_mat, f1, f2, f3)\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer.\n",
    "\n",
    "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "    with tf.name_scope(layer_name):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_variable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = act(preactivate, name='activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _write_sample_file(fp, data):\n",
    "    with open(fp, 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='|')\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "def _read_sample_file(fp):\n",
    "    with open(fp, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter='|')\n",
    "        for row in reader:\n",
    "            yield row\n",
    "            \n",
    "def _regen_samples():\n",
    "    def _gen(num):\n",
    "        for _ in range(num):\n",
    "            nodes, adj_mat = gen_graph(SAMPLE_GRAPH_NODES, SAMPLE_GRAPH_EDGES, directed=True)\n",
    "            yield str(nodes), ','.join(str(v) for v in np.matrix.flatten(adj_mat))\n",
    "\n",
    "    train_data = _gen(SAMPLES_TRAIN)\n",
    "    _write_sample_file(_SAMPLE_TRAIN_FILE, train_data)\n",
    "    \n",
    "    validate_data = _gen(SAMPLES_VALIDATE)\n",
    "    _write_sample_file(_SAMPLE_VALIDATE_FILE, validate_data)\n",
    "    \n",
    "    test_data = _gen(SAMPLES_TEST)\n",
    "    _write_sample_file(_SAMPLE_TEST_FILE, test_data)\n",
    "            \n",
    "\n",
    "if (\n",
    "    not os.path.exists(_SAMPLE_TRAIN_FILE) \n",
    "    or not os.path.exists(_SAMPLE_VALIDATE_FILE)\n",
    "    or not os.path.exists(_SAMPLE_TEST_FILE)\n",
    "    or FORCE_REGEN_SAMPLE\n",
    "):\n",
    "    #_regen_samples()\n",
    "    pass\n",
    "    \n",
    "\n",
    "def sample_batch_from_file(fp, batch_size=100):\n",
    "    done = False\n",
    "    reader = _read_sample_file(fp)\n",
    "    while not done:\n",
    "        batch_adj_mat_with_f, batch_y = [], []\n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                adj_mat, *f = parse_sample(next(reader))\n",
    "                bf = adj_mat\n",
    "                ident = np.identity(f[0])\n",
    "                bf = np.asarray([bf])\n",
    "                batch_adj_mat_with_f.append(bf)\n",
    "                batch_y.append(f)\n",
    "            except StopIteration:\n",
    "                done = True\n",
    "                del reader\n",
    "                break\n",
    "        yield batch_adj_mat_with_f, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, name=\"weights\")\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(1.0, shape=shape, name=\"bias\")\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    # tf.nn.conv2d对一个四维输入做二维卷积。\n",
    "    # 参数分别为输入，过滤器，步长，补0策略\n",
    "    # 其中：\n",
    "    # 输入的四个维度分别为：样本数，每样本高度，每样本宽度，通道数\n",
    "    # 过滤器的四个维度分别为：过滤器高度，过滤器宽度，输入通道数，输出通道数\n",
    "    # 步长为一个一维四元素张量，每元素分别表示在输入的对应维度上的步长\n",
    "    # 补0策略为'SAME'或'VALID'\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name=\"conv2d\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # tf.nn.max_pool对输入进行最大池化\n",
    "    # 参数分别为输入，窗口大小，步长，补0策略\n",
    "    # \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=\"max pool 2x2\")\n",
    "\n",
    "max_nodes = SAMPLE_GRAPH_NODES[1]\n",
    "features = 1\n",
    "\n",
    "# Input adjacency matrices (samples, height, width)\n",
    "input_adj_mat_with_f = tf.placeholder(tf.float32, [None, features, 10, 10])\n",
    "input_flat = tf.reshape(input_adj_mat_with_f, [-1, features * 100])\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "# First layer\n",
    "l1 = nn_layer(input_flat, features * 100, 300, \"first\", act=tf.nn.relu)\n",
    "\n",
    "# Second layer\n",
    "l2 = nn_layer(l1, 300, 64, \"second\", act=tf.nn.relu)\n",
    "\n",
    "# Third layer\n",
    "l3 = nn_layer(l2, 64, 64, \"third\", act=tf.nn.relu)\n",
    "\n",
    "# h_pool2 = max_pool_2x2(h_2)\n",
    "\n",
    "# Densely connected layer\n",
    "ld = nn_layer(l3, 64, 1024, \"dense\", act=tf.nn.relu)\n",
    "\n",
    "# Dropout\n",
    "with tf.name_scope('dropout') as scope:\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_d_drop = tf.nn.dropout(ld, keep_prob, name=\"dropout\")\n",
    "\n",
    "# Readout layer\n",
    "lr = nn_layer(h_d_drop, 1024, 3, \"readout\", act=tf.nn.relu)\n",
    "\n",
    "y_conv = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "#cross_entropy = tf.reduce_mean(\n",
    "#    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=relu_3))\n",
    "c1 = y_ - y_conv\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(tf.reduce_sum(tf.square(c1)))\n",
    "#train_step = tf.train.GradientDescentOptimizer(1e-5).minimize(tf.reduce_sum(tf.square(c1)))\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.cast(tf.round(y_conv), tf.int32), tf.cast(y_, tf.int32))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# launch the model\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('train',\n",
    "                                      sess.graph)\n",
    "test_writer = tf.summary.FileWriter('test')\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: accuracy: 0.0333333\n",
      "Step 100: accuracy: 0.266667\n",
      "Step 200: accuracy: 0.34\n",
      "Step 300: accuracy: 0.506667\n",
      "Step 400: accuracy: 0.573333\n",
      "Step 500: accuracy: 0.52\n",
      "Step 600: accuracy: 0.533333\n",
      "Step 700: accuracy: 0.613333\n",
      "Step 800: accuracy: 0.66\n",
      "Step 900: accuracy: 0.62\n",
      "Step 1000: accuracy: 0.653333\n",
      "Step 1100: accuracy: 0.66\n",
      "Step 1200: accuracy: 0.64\n",
      "Step 1300: accuracy: 0.653333\n",
      "Step 1400: accuracy: 0.686667\n",
      "Step 1500: accuracy: 0.713333\n",
      "Step 1600: accuracy: 0.713333\n",
      "Step 1700: accuracy: 0.726667\n",
      "Step 1800: accuracy: 0.706667\n",
      "Step 1900: accuracy: 0.713333\n",
      "Step 2100: accuracy: 0.726667\n",
      "Step 2200: accuracy: 0.76\n",
      "Step 2300: accuracy: 0.72\n",
      "Step 2400: accuracy: 0.753333\n",
      "Step 2500: accuracy: 0.713333\n",
      "Step 2600: accuracy: 0.7\n",
      "Step 2700: accuracy: 0.74\n",
      "Step 2800: accuracy: 0.693333\n",
      "Step 2900: accuracy: 0.72\n",
      "Step 3000: accuracy: 0.753333\n",
      "Step 3100: accuracy: 0.753333\n",
      "Step 3200: accuracy: 0.713333\n",
      "Step 3300: accuracy: 0.746667\n",
      "Step 3400: accuracy: 0.713333\n",
      "Step 3500: accuracy: 0.733333\n",
      "Step 3600: accuracy: 0.746667\n",
      "Step 3700: accuracy: 0.74\n",
      "Step 3800: accuracy: 0.74\n",
      "Step 3900: accuracy: 0.713333\n",
      "Step 4000: accuracy: 0.753333\n",
      "Step 4100: accuracy: 0.68\n",
      "Step 4200: accuracy: 0.713333\n",
      "Step 4300: accuracy: 0.72\n",
      "Step 4400: accuracy: 0.78\n",
      "Step 4500: accuracy: 0.62\n",
      "Step 4600: accuracy: 0.786667\n",
      "Step 4700: accuracy: 0.833333\n",
      "Step 4800: accuracy: 0.773333\n",
      "Step 4900: accuracy: 0.76\n",
      "Step 5000: accuracy: 0.76\n",
      "Step 5100: accuracy: 0.766667\n",
      "Step 5200: accuracy: 0.713333\n",
      "Step 5300: accuracy: 0.78\n",
      "Step 5400: accuracy: 0.72\n",
      "Step 5500: accuracy: 0.786667\n",
      "Step 5600: accuracy: 0.713333\n",
      "Step 5700: accuracy: 0.706667\n",
      "Step 5800: accuracy: 0.773333\n",
      "Step 5900: accuracy: 0.746667\n",
      "Step 6000: accuracy: 0.78\n",
      "Step 6100: accuracy: 0.72\n",
      "Step 6200: accuracy: 0.746667\n",
      "Step 6300: accuracy: 0.773333\n",
      "Step 6400: accuracy: 0.72\n",
      "Step 6500: accuracy: 0.726667\n",
      "Step 6600: accuracy: 0.793333\n",
      "Step 6700: accuracy: 0.68\n",
      "Step 6800: accuracy: 0.733333\n",
      "Step 6900: accuracy: 0.78\n",
      "Step 7000: accuracy: 0.753333\n",
      "Step 7100: accuracy: 0.766667\n",
      "Step 7200: accuracy: 0.766667\n",
      "Step 7300: accuracy: 0.773333\n",
      "Step 7400: accuracy: 0.74\n",
      "Step 7500: accuracy: 0.513333\n",
      "Step 7600: accuracy: 0.773333\n",
      "Step 7700: accuracy: 0.733333\n",
      "Step 7800: accuracy: 0.733333\n",
      "Step 7900: accuracy: 0.76\n",
      "Step 8000: accuracy: 0.733333\n",
      "Step 8100: accuracy: 0.793333\n",
      "Step 8200: accuracy: 0.74\n",
      "Step 8300: accuracy: 0.753333\n",
      "Step 8400: accuracy: 0.733333\n",
      "Step 8500: accuracy: 0.746667\n",
      "Step 8600: accuracy: 0.713333\n",
      "Step 8700: accuracy: 0.753333\n",
      "Step 8800: accuracy: 0.7\n",
      "Step 8900: accuracy: 0.746667\n",
      "Step 9000: accuracy: 0.8\n",
      "Step 9100: accuracy: 0.773333\n",
      "Step 9200: accuracy: 0.746667\n",
      "Step 9300: accuracy: 0.746667\n",
      "Step 9400: accuracy: 0.72\n",
      "Step 9500: accuracy: 0.733333\n",
      "Step 9600: accuracy: 0.766667\n",
      "Step 9700: accuracy: 0.746667\n",
      "Step 9800: accuracy: 0.753333\n",
      "Step 9900: accuracy: 0.773333\n",
      "Step 10000: accuracy: 0.786667\n",
      "Step 10100: accuracy: 0.76\n",
      "Step 10200: accuracy: 0.76\n",
      "Step 10300: accuracy: 0.74\n",
      "Step 10400: accuracy: 0.773333\n",
      "Step 10500: accuracy: 0.493333\n",
      "Step 10600: accuracy: 0.733333\n",
      "Step 10700: accuracy: 0.766667\n",
      "Step 10800: accuracy: 0.713333\n",
      "Step 10900: accuracy: 0.686667\n",
      "Step 11000: accuracy: 0.753333\n",
      "Step 11100: accuracy: 0.793333\n",
      "Step 11200: accuracy: 0.733333\n",
      "Step 11300: accuracy: 0.78\n",
      "Step 11400: accuracy: 0.746667\n",
      "Step 11500: accuracy: 0.733333\n",
      "Step 11600: accuracy: 0.786667\n",
      "Step 11700: accuracy: 0.813333\n",
      "Step 11800: accuracy: 0.74\n",
      "Step 11900: accuracy: 0.766667\n",
      "Step 12000: accuracy: 0.773333\n",
      "Step 12100: accuracy: 0.746667\n",
      "Step 12200: accuracy: 0.76\n",
      "Step 12300: accuracy: 0.74\n",
      "Step 12400: accuracy: 0.76\n",
      "Step 12500: accuracy: 0.8\n",
      "Step 12600: accuracy: 0.766667\n",
      "Step 12700: accuracy: 0.793333\n",
      "Step 12800: accuracy: 0.793333\n",
      "Step 12900: accuracy: 0.766667\n",
      "Step 13000: accuracy: 0.82\n",
      "Step 13100: accuracy: 0.76\n",
      "Step 13200: accuracy: 0.753333\n",
      "Step 13300: accuracy: 0.753333\n",
      "Step 13400: accuracy: 0.74\n",
      "Step 13500: accuracy: 0.746667\n",
      "Step 13600: accuracy: 0.746667\n",
      "Step 13700: accuracy: 0.733333\n",
      "Step 13800: accuracy: 0.773333\n",
      "Step 13900: accuracy: 0.74\n",
      "Step 14000: accuracy: 0.7\n",
      "Step 14100: accuracy: 0.753333\n",
      "Step 14200: accuracy: 0.78\n",
      "Step 14300: accuracy: 0.72\n",
      "Step 14400: accuracy: 0.72\n",
      "Step 14500: accuracy: 0.793333\n",
      "Step 14600: accuracy: 0.78\n",
      "Step 14700: accuracy: 0.733333\n",
      "Step 14800: accuracy: 0.76\n",
      "Step 14900: accuracy: 0.78\n",
      "Step 15000: accuracy: 0.766667\n",
      "Step 15100: accuracy: 0.813333\n",
      "Step 15200: accuracy: 0.773333\n",
      "Step 15300: accuracy: 0.68\n",
      "Step 15400: accuracy: 0.746667\n",
      "Step 15500: accuracy: 0.76\n",
      "Step 15600: accuracy: 0.793333\n",
      "Step 15700: accuracy: 0.74\n",
      "Step 15800: accuracy: 0.766667\n",
      "Step 15900: accuracy: 0.76\n",
      "Step 16000: accuracy: 0.793333\n",
      "Step 16100: accuracy: 0.753333\n",
      "Step 16200: accuracy: 0.8\n",
      "Step 16300: accuracy: 0.82\n",
      "Step 16400: accuracy: 0.76\n",
      "Step 16500: accuracy: 0.766667\n",
      "Step 16600: accuracy: 0.786667\n",
      "Step 16700: accuracy: 0.793333\n",
      "Step 16800: accuracy: 0.766667\n",
      "Step 16900: accuracy: 0.753333\n",
      "Step 17000: accuracy: 0.76\n",
      "Step 17100: accuracy: 0.733333\n",
      "Step 17200: accuracy: 0.726667\n",
      "Step 17300: accuracy: 0.753333\n",
      "Step 17400: accuracy: 0.786667\n",
      "Step 17500: accuracy: 0.76\n",
      "Step 17600: accuracy: 0.74\n",
      "Step 17700: accuracy: 0.753333\n",
      "Step 17800: accuracy: 0.78\n",
      "Step 17900: accuracy: 0.766667\n",
      "Step 18000: accuracy: 0.773333\n",
      "Step 18100: accuracy: 0.766667\n",
      "Step 18200: accuracy: 0.726667\n",
      "Step 18300: accuracy: 0.76\n",
      "Step 18400: accuracy: 0.726667\n",
      "Step 18500: accuracy: 0.76\n",
      "Step 18600: accuracy: 0.773333\n",
      "Step 18700: accuracy: 0.8\n",
      "Step 18800: accuracy: 0.766667\n",
      "Step 18900: accuracy: 0.753333\n",
      "Step 19000: accuracy: 0.766667\n",
      "Step 19100: accuracy: 0.753333\n",
      "Step 19200: accuracy: 0.72\n",
      "Step 19300: accuracy: 0.766667\n",
      "Step 19400: accuracy: 0.786667\n",
      "Step 19500: accuracy: 0.746667\n",
      "Step 19600: accuracy: 0.726667\n",
      "Step 19700: accuracy: 0.733333\n",
      "Step 19800: accuracy: 0.62\n",
      "Step 19900: accuracy: 0.726667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch = sample_batch_from_file(_SAMPLE_TRAIN_FILE, batch_size=BATCH)\n",
    "\n",
    "for i in range(SAMPLES_TRAIN // BATCH * 10):\n",
    "    try:\n",
    "        batch_adj_mat_with_f, batch_y = next(batch)\n",
    "    except StopIteration:\n",
    "        batch = sample_batch_from_file(_SAMPLE_TRAIN_FILE, batch_size=BATCH)\n",
    "        batch_adj_mat_with_f, batch_y = next(batch)\n",
    "    if len(batch_adj_mat_with_f) == 0:\n",
    "        continue\n",
    "    if i % 100 == 0:\n",
    "        summary, acc = sess.run([merged, accuracy], feed_dict={input_adj_mat_with_f: batch_adj_mat_with_f, \n",
    "                                       y_: batch_y, keep_prob: 1})\n",
    "        test_writer.add_summary(summary, i)\n",
    "        print(\"Step %d: accuracy: %g\" % (i, acc))\n",
    "    summary, _ = sess.run([merged, train_step], feed_dict={input_adj_mat_with_f: batch_adj_mat_with_f, \n",
    "                                    y_: batch_y, keep_prob: 0.5})\n",
    "    train_writer.add_summary(summary, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_validate = sample_batch_from_file(_SAMPLE_TEST_FILE, batch_size=20)\n",
    "\n",
    "batch_adj_mat_val_with_f, batch_y_val = next(batch_validate)\n",
    "result = sess.run(y_conv, feed_dict={input_adj_mat_with_f: batch_adj_mat_val_with_f,\n",
    "                                       y_: batch_y_val, keep_prob: 1\n",
    "                                       })\n",
    "\n",
    "for idx, x in enumerate(batch_y_val):\n",
    "    print(\"Expected: %s; Actual: %s\" % (x, result[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
